\documentclass[12,twoside]{mammeTFM}
%\usepackage[active]{srcltx}
\usepackage{amsthm} % To make environments with different styles.
\usepackage{amsmath,amssymb,amsfonts} % Multiple mathematics symbols and fonts.
%\usepackage{amscd} % To make commutative diagrams
\usepackage{graphicx} % To include figures in a simple way. Fancy options can be found for example in http://www.kwasan.kyoto-u.ac.jp/solarb6/usinggraphicx.pdf
\usepackage{enumerate} % It allows you to make list with specific somehow arbitrary labels, like this one.
%\usepackage[all]{xy} % To make really fancy commutative diagrams
\usepackage{booktabs} % To make fancy tables.
%\usepackage[usenames]{xcolor}
%\usepackage{fancyhdr}

%%%%% My packages (Should probably push all the header to a certain file and include it in each latex.

%\usepackage[left=15mm, right=15mm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{bigfoot}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{epstopdf}
\usepackage{vhistory}
\usepackage{framed}
\usepackage{hyperref}
\usepackage{cancel}

\setlength{\parskip}{11pt}

% Theorem Environments: add extra ones at the end if you need it.
\newtheorem*{thmA}{Theorem A}
\newtheorem{thm}{Theorem}[section]

\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{conj}[thm]{Conjecture}

\theoremstyle{definition}
\newtheorem{definition}[thm]{Definition}
\newtheorem{exmp}[thm]{Example}

\theoremstyle{remark}
\newtheorem{remark}[thm]{Remark}
\newtheorem*{remarknonumber}{Remark}
\newtheorem{observation}[thm]{Observation}
%%% My own theorems
\newtheorem{main_thm}[thm]{Main Theorem}

%% rme, rmi e Id son el número e, el número imaginario i y la identidad respectivamente. Poned ds antes de una expresión
%% cuando salga mal (en las fracciones y cosas parecidas)
\def\ds{\displaystyle}
\def\rme{\mathrm{e}}
\def\rmi{\mathrm{i}}
\def\Id{\mathrm{Id}}
\def\resposta{\bullet\bullet\bullet\bullet\bullet\bullet}

%%%%%%%%%%%%%%%%%%
% macros/abbreviations: Include here your own.
%%%%%%%%%%%%%%%%%%

%% Conjuntos típicos.
\newcommand{\N}{\ensuremath{\mathbb{N}}}
\newcommand{\E}{\ensuremath{\mathbb{E}}}
\newcommand{\Z}{\ensuremath{\mathbb{Z}}}
\newcommand{\Q}{\ensuremath{\mathbb{Q}}}
\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\C}{\ensuremath{\mathbb{C}}}
\newcommand{\F}{\ensuremath{\mathbb{F}}}
\newcommand{\MP}{\ensuremath{\mathbb{P}}}

% math stuff
\newcommand{\Expect}{\ensuremath{{\rm I\kern-.3em E}}}
\newcommand{\Var}{\ensuremath{\mathrm{Var}}}
\newcommand{\Cov}{\ensuremath{\mathrm{Cov}}}

% random
\makeatletter
\newcommand{\raisemath}[1]{\mathpalette{\raisem@th{#1}}}
\newcommand{\raisem@th}[3]{\raisebox{#1}{$#2#3$}}
\makeatother

\newcommand{\function}[5]{\begin{align*} #1\colon #2 &\to #3 \\ #4 &\mapsto #5\end{align*}}
\newcommand{\vega}{\nu}

%% Separacio correcta paraules final de linia.
\hyphenation{Bar-ce-lo-na}

%% Ejemplo de como hacer una tabla.

%\begin{tabular}{|r|c|c|}
%\hline
%$n$	&nº bits (estándar)	&nº bits (\emph{sp. triplet})\\
%\hline
%10   	  &  6400       	& 3040\\
%\hline
%50   	  & 160000    	& 78400\\
%\hline
%100 	  & 640000    	& 314320\\
%\hline
%200 	  & 2560000  	& 1262320\\
%\hline
%400 	  & 10240000	&5043680\\
%\hline
%650 	  & 27040000	&13282720\\
%\hline
%1000 	  & 64000000	&31479040\\
%\hline
%\end{tabular}


%% Ejemplo de como hacer una figura

%\begin{figure}[ht]
%\centering
%\includegraphics[width=6cm,angle=-90]{exemple.eps}
%\caption{Corba de continuaci\'o d'$u_2$ com a funci\'o de $\lambda$.}
%\label{fig:etiqueta}
%\end{figure} 

%% se puede hacer \input{archivo.tex} en lugar de \includegraphics

%% Y de como hacerle referencia

% bla bla bla en la Figura~\ref{fig:etiqueta}


% Body of document

\titol{This is the long title\\[3mm] with a line skip}
\titolcurt{Heston Calibration using SWIFT}
\authorStudent{Eudald Romo Grau}
\supervisors{(name of the supervisor/s of the master's thesis)}
\monthYear{Month, year}

%\msc[2010]{Primary  	55M25, 57P10, Secondary 55P15, 57R19, 57N15.}

\paraulesclau{Derivatives Trading, Heston, SWIFT, calibration}
\agraiments{
Thanks to...}


\abstracteng{This should be an abstract in English, up to 1000 characters.}

%%%%%%%%%
\begin{document}

\maketitle

\tableofcontents

\pagebreak

% The expected structure is explained in https://bibliotecnica.upc.edu/estudiants/6-passos-que-teu-tfg/tfm-sigui-exit/escric-meu-tfg/tfm#criteris-grafics

\section{Motivation}

Option pricing has an important role in contract trading, both as a form of derivative trading in itself, as a way to hedge other stock or derivative portfolios.

Pricing these options can have a broad range of difficulty, mainly due to two factors:
\begin{enumerate}[\bf (1)]
\item {\tt Contract type: } These options can take a variety of forms, from the vanilla European Options to more complex American or Asian options (TODO: citations).
\item {Underlying Model: } The price of the derivative depends on the future price of its underlying. The evolution of this price can be modeled using simpler models like Black \& Scholes, or using other models that take into account more theoretical results related to the underlying price evolution, like the Heston model. {TODO: citations, add more models.}
\end{enumerate}

There are only closed analytic solutions for the Option pricing problem for the simpler contracts and models: European options under BS and can have no analytic solution when one either trades a more complicated contract (like American options) or uses a more complex model (like Heston) to model its dynamics.

The advantages of simpler models tend to be faster computations and possibly analytic functions to compute the price. On the other hand, they fail to capture more complex behavior of the market.

For example, BS is probably the most used model in real option trading (TODO: back this up) but fails to capture the well-known high kurtosis and negative skewness of the log returns volatility, which motivated the development of the Heston model, which will be the center of this study.

Most of these models can be used to price different strikes using a set of intrinsic parameters. Usually this parameters are calibrated either using historical data or using data from options with the same underlying at different strikes and, sometimes, different maturities. (TODO: Cite) In the second calibration mode, which is usually used in the trading world, particularly in high frequency trading, (TODO: cite) fast option calculation methods and model calibration techniques are required to be able to update the model in real time.

In particular, solving the Heston model calibration is a difficult optimization problem to solve, partly due to it being a multidimensional optimization problem with a mainly unknown structure: it is not known whether it is a convex problem or not nor whether it generally has a single solution or not. Because of that, several different branches of research have appeared, but they are either deterministic but slow, or use heuristics or assumptions about the model parameters to reduce the dimension of the problem.

A branch of numerical option pricing that, given a density function (and the value of its parameters) to describe the stochastic evolution of the underlying over time, use its characteristic function to valuate the price of the option with different strikes and maturities.

Traditionally, using this pricing method combined with a gradient-descent based method for calibration was not possible due to the characteristic function derivatives with respect of the model parameters being too intractable.

Recently, there has been both work in fast option pricing numerical techniques using characteristic functions (TODO: cite) and giving simple expressions for the characteristic function (TODO: cite) of the Heston model (its previous expressions either had complex variable discontinuities that translated in numeric problems (TODO: cite) or complicated derivative expressions, which made it difficult to use in gradient descent based methods).

This work is centered around the calibration of the Heston model for European Options (choosing proper values of the model parameters to minimize the option valuation error) by using a gradient descent method (TODO: name properly) valuating the option and its partial derivatives using the new characteristic function approach in the SWIFT setup.

\section{Nomenclature}

I will probably not add this section (that's why it doesn't have fancy formatting), but it's good to have everything set recorded somewhere and I might finally decide to polish it up and include this section.

$t$ will refer to the time at which the option is being valuated

$T$ will be the expiration time. Given a time-dependent variable $Var$, $Var_t$ will refer to the value of that variable at time $t$ (respectively with $T$). 

$S, K$ will refer to the underlying and the strike prices respectively.

$\tau := T - t$

$r$ will refer to the risk-free interest rate.

$q$ will refer to the dividend rate.

\section{Option Valuation}

\begin{definition}
An European Option is a derivative contract on an underlying asset that fixes a transaction price (called \textbf{strike price}), an expiration time (also called \textbf{maturity}), and an \textbf{underlying} asset quantity and gives the holder of the contract the right (but not the obligation) of buying (Call Option) or selling (Put Option) the specified quantity of the underlying at the Strike price on expiration.
\end{definition}

In the following sections we will use the following nomenclature for Option parameters: $t$ will refer to the time at which the option is being valuated and $T$ will be the expiration time. Given a time-dependent variable $V$, $V_t$ will refer to the value of that variable at time $t$ (respectively with $T$). $S, K$ will refer to the underlying and the strike prices respectively.

Slight variations of these terms can give rise to other families of options by facilitating the exercising of the contracts, like the Bermudan and American options (which allow to exercise your buying/selling right at specified instants of time or at any point prior to expiry, respectively).

Other groups of option families are obtained by changing the definition of the option payoff, that is, the amount of money (be it in cash or in the value of the underlying obtained) the option gives on expiration.

\begin{definition} Payoff of a Call European option:
\begin{equation}
V_{EC}(y, T) = (S_T - K)\boldsymbol{1}_{S_T \geq K} = \left\{ \begin{array}{rcl}
S_T - K & \mbox{if} & S_T > K \\ 
0 & \mbox{if} & S_T\leq K
\end{array}\right.
\end{equation} 
\end{definition}

So, on maturity, a European Call option is only profitable if it allows the holder of the contract to buy the underlying asset at a cheaper price than the current asset spot price. In most models any option has a certain positive value if it's not expired, as the underlying price can always theoretically move enough so that a currently non-profitable option ends up with a positive payoff on expiry. Another 

\begin{definition} A Call European option at time $t$ is called:
\begin{itemize}
\item {Out of the money (OTM):} If $K > S_t$ (that is, it has no value apart from time value).
\item {At the money (ATM):} If $K = S_t$ (the price of a real option contract will rarely match perfectly the underlying asset price, but one can refer to a theoretic ATM option, for example to describe a volatility surface).
\item {In the money (ITM):} If $K < S_t$ (that is, even if it were to expire right now, it would still hold some implicit value).
\end{itemize}

\end{definition}

Some examples of other families of functions that have different payoffs are Binary Options (only have two possible payoffs dependent on $K$ and $S_T$) and Asian Options (the payoff depends on the mean price of the underlying since the start of the contract and until maturity).

In order to price an option that expires at some point in the future, one usually first models the temporal evolution of the price of its underlying asset by using some stochastic process. This model will typically be expressed in terms of parameters that will be calibrated to fit the underlying price evolution of each product one wants to valuate.
% TODO: Some discussion on how the nature of the contract affects the difficulty of pricing it.

\subsection{No arbitrage and risk neutrality assumptions.}

Put-Call parity (that's why we only talk about Calls in this article)
Model
Log-Returns

\subsection{European Option Valuation}
European options payoff only depends on the final price of the underlying asset on the moment of expiration of the contract. Thus, if we model the underlying asset's price evolution with an stochastic process, we can express the stochastic distribution of the payoff, and using risk neutrality, compute the option price as the discounted expectation (TODO: Explain in \ref{subsec:riskneu}) (under risk neutrality assumptions) of the payoff:
\begin{align} 
\label{eq:option_valuation_expectation}
& v(x, \tau) & = e^{-r \tau}\E^{\Q}(v(y, T)|x) & = e^{-r \tau} \E^{Q}((S_T - K)\boldsymbol{1}_{S_T \geq K}) \\  
\label{eq:option_valuation_useless}
&  &  & = e^{-r \tau}(\E^{Q}(S_T \boldsymbol{1}_{S_T \geq K}) - K\E^{Q}(\boldsymbol{1}_{S_T \geq K})) \\
\label{eq:option_valuation_final_form}
&  &  & = S_t e^{-q \tau} P_1(x, \tau) - K e^{-r \tau} P_2(x, \tau)
\end{align}

, where v denotes the option value, r the risk-neutral interest rate, $\E^Q$ the expectation under the risk-neutral measure, x and y denote state variables that fully describe the underlying asset process state at times t and T respectively, and $P_1$ and $P_2$ are specific to the model we are using (their expression for the Heston model will be provided in \ref{chap:heston_model}.

The expectation from equation \ref{eq:option_valuation_expectation}
, can be expressed as the following integral, which can then be approximated numerically:

\begin{equation}\label{eq:integral_option_valuation}
v(x,t) = e^{-r(T - t)}\E^{\Q}(v(y, T)|x) = e^{-r(t - T)} \int_{\R}v(y,T)f(y|x)dy
\end{equation}

There is a broad literature in methods to solve this integration problem for basic underlying asset models and some simpler models, as Black and Scholes even have closed solutions (TODO: ref BS formula) but it is common that, for more complex models, the probability function at expiry doesn't have a known expression but it's characteristic function does, so several methods have been developed using numerical integration and the Fourier transform.

\subsection{Volatility smile and surface} \label{subsec:smile_and_surf}

\section{Black and Scholes} \label{sec:bs}

\section{L\'evy Processes in Option Pricing}
Some well known underlying asset log returns theoretical stochastic processes, as the Geometric Brownian Motion (GBM) model (known as the Black-Scholes-Merton\cite{bs73, mer73} model), can be considered inside a more general concept called L\'evy processes, which have a known Fourier transform.

\begin{definition} A stochastic process $X = \{X_t : t \geq 0\}$ is considered a L\'evy process if:
\begin{itemize}
\item $X_0 = 0$ almost surely.
\item For any $0 \leq t_1 \leq t_2 \leq \cdots \leq t_n \leq \infty$, $X_{t_2} - X_{t_1}, X_{t_3} - X{t_2}, \cdots X{t_n} - X_{t_{n-1}}$ are independent.
\item $\forall s < t$, $X_t - X_s$ is equal in distribution to $X_{t-s}$
\end{itemize}
\end{definition}

A famous property of L\'evy processes is the L\'evy-Khintchine formula:
\begin{lem}\label{levy_khin} Let $X = (X_t)_{t\geq 0}$ be a L\'evy process. Then its characteristic function $Char_X(\omega)$ is:
$$
Char_X(\omega)(t) := \E \left[e^{i\theta X(t)}\right] = e^{t\left(ai\omega - \dfrac{\sigma^2 \omega^2}{2} + \int_{\{0\}^c}(e^{i\omega x} \textbf{I}_{|x| < 1})\Pi(dx)\right)} = e^{\psi_L(w)}
$$
, where $\psi_L(\omega)$ is called the characteristic L\'evy exponent, and $a \in \R$, $\sigma \geq 0$, and $\Pi$ is the L\'evy measure of X, a $\sigma$-finite measure satisfying $\int_{\{0\} ^c}(1 \wedge x^2)\Pi (dx) < \infty$
\end{lem}

Note that any L\'evy process can be completely characterized by the triplet $(a, \sigma, \Pi)$, which represent a linear drift, a Brownian motion, and a L'evy jump process respectively.

\subsection{Underlying Process Modeling}

In option pricing, one can denote by $\{S_t\}_{t=0}^T$ the underlying price process and model it with an exponential L\'evy process. For a given strike $K$, one can consider the log-transformed process $X_t := log(U_t / K)$, which will follow another L\'evy process with an extra drift. Let the state variables be $x := log(S_t / K)$ and $y := log(U_T / K)$. In order to solve equation \ref{eq:integral_option_valuation} one needs to evaluate $f(y|x)$, which is not always available in analytic form, but formula \ref{levy_khin} can be used to obtain the form of its Fourier transform at any strike: 
%TODO Make sure the term at the money has been introduced before.
$$
\hat{f}(\theta; x) := \E\left[e^{i\theta X_T}\right] = e^{-i\theta x}e^{-i \theta \mu T + T \psi_L(-\theta)} =: e^{-i\theta x}\hat{f}(\theta) 
$$
%TODO Make sure I understand wether this expresses the general one in terms of the ATM one, or gives an expression for 
, where $\mu:= r - \psi_L(-i)$ is a drift correction term and $\psi_L(\theta)$is the characteristic L'evy exponent of the underlying log-returns process.

Given this price transformation, we can express the payoff of a put option as
$$
v(y, T) := K \cdot max(1 - e^y, 0)
$$
It is usually better for numerical reasons to always calculate the price of the call via the price of the put and the put-call parity formula, as $e^y$ can have arbitrarily large values inside the integration domain.
%TODO I don't know how relevant that is



\begin{table}\label{tab:levy_exp}
\center
\begin{tabular}{|l|l|c|}
\hline
Model     & $\psi_L(w)$	& Param. Restrictions \\
\hline
GBM   	  &  $-\dfrac{\sigma^2}{2}\omega^2$	& $\sigma > 0$ \\
\hline
\end{tabular}
\caption{Test}
\end{table}
\section{Heston Model} \label{chap:heston_model}

The Black and Scholes model discussed in section \ref{sec:bs} fails to capture essential well known properties of the real world market dynamics of the log returns distributions, as its high kurtosis, its negative skew, and the correlation between the underlying price and its volatility.

Several variations of this model have been proposed since it was introduced. Some of them, called stochastic volatility (SV) models, considered all the real world dynamics mentioned above and treated both the underlying price and its volatility as (potentially correlated) stochastic processes.

One of the first and most well-known SV models is the Heston model, defined by the following system of stochastic differential equations.

\begin{definition} Heston model price-volatility equations:
$$
dS_t = \mu S_t dt + \sqrt{\nu_t} S_t W_t^{(1)}
$$
$$
d\nu_t = \kappa(\overline{\nu} - \nu_t) dt + \sigma \sqrt{\nu_t}dW_t^{(2)}
$$
$$
dW_t^{(1)}dW_t^{(2)} = \rho dt
$$
, where $\nu_t$ is the variance of the underlying asset price at time t. The parameters $\kappa$, $\overline{\nu}, \sigma, \rho$ are respectively called: mean-reversion rate, long-term variance, volatility of volatility, and correlation between the Brownian processes $W_t^{(1)}$ and $W_t^{(2)}$. From now on, $\theta := (\nu_0, \overline{\nu}, \rho, \kappa, \sigma)$ will refer to the set of model parameters.
\end{definition}

Several studies have shown the relations between the Heston model parameters and the shape of the implied volatility surface \cite{cla11, gat06, gil12, jan11} which can be summarized as follows:

\begin{itemize} \label{hes_param_properties}
\item $\nu_0$ controls the position of the volatility surface.
\item $\rho$ controls its skewness.
\item $\kappa$ and $\sigma$ control the convexity of the surface.
\item $\kappa(\nu_0 - \overline{\nu})$ controls the term structure of implied volatility. %TODO look this up.
\end{itemize}

Heston also provided an expression of the price of an European option. In terms of equation \ref{eq:option_valuation_final_form}, the values of $P_1$ and $P_2$ are:

\begin{equation}
\begin{aligned}
&P_{1}(\boldsymbol{\theta} ; K, \tau)=\frac{1}{2}+\frac{1}{\pi} \int_{0}^{\infty} \operatorname{Re}\left(\frac{e^{-i u \log \frac{K}{5_{0}}}}{i u} \frac{\phi(\boldsymbol{\theta} ; u-i, \tau)}{\phi(\boldsymbol{\theta} ;-i, \tau)}\right) \mathrm{d} u\\
&P_{2}(\boldsymbol{\theta} ; K, \tau)=\frac{1}{2}+\frac{1}{\pi} \int_{0}^{\infty} \operatorname{Re}\left(\frac{e^{-i u \log \frac{K}{5_{0}}}}{i u} \phi(\boldsymbol{\theta} ; u, \tau)\right) \mathrm{d} u
\end{aligned}
\end{equation}

Substituting these values into equation \ref{eq:option_valuation_final_form} , one can obtain an analytic equation to obtain the price of European Call options:

\begin{lem} Heston's pricing method.
\begin{equation} \label{eq:heston_analytic}
\begin{aligned}
C(\theta ; K, \tau)=& \frac{1}{2}\left(S_{t} e^{-q \tau}-K e^{-r \tau}\right) \\
&+\frac{e^{-r \tau}}{\pi}\left[S_{0} \int_{0}^{\infty} \operatorname{Re}\left(\frac{e^{-i u \log \frac{K}{S_0}}}{i u} \phi(\theta ; u-i, \tau)\right) \mathrm{d} u\right. \\
&\left.-K \int_{0}^{\infty} \operatorname{Re}\left(\frac{e^{-i u \log \frac{K}{S_0}}}{i u} \phi(\theta ; u, \tau)\right) \mathrm{d} u\right]
\end{aligned}
\end{equation}

, where $\phi$ is the characteristic function of the process followed by the logarithm of price of the underlying asset. The expression of this characteristic function is
\begin{equation} \label{eq:char_heston}
\begin{array}{l}
\phi(\boldsymbol{\theta} ; u, \tau):=\mathrm{E}\left[\exp \left(i u \log \frac{S_{t}}{S_{0}}\right)\right]=\exp \left\{i u \log \frac{F}{S_{0}}\right. \\
\left.\quad+\frac{\kappa \bar{v}}{\sigma^{2}}\left[(\xi+d) \tau-2 \log \frac{1-g_{1} e^{d \tau}}{1-g_{1}}\right]+\frac{v_{0}}{\sigma^{2}}(\xi+d) \frac{1-e^{d \tau}}{1-g_{1} e^{d \tau}}\right\}
\end{array}
\end{equation}
, where $F := S_t e^{(r - q) \tau}$ is the forward price and

\noindent\begin{minipage}{.3\linewidth}
\begin{equation}
\xi:=\kappa-\sigma \rho i u
\end{equation}
\end{minipage}
\noindent\begin{minipage}{.4\linewidth}
\begin{equation} \label{eq:char_d}
d:=\sqrt{\xi^{2}+\sigma^{2}\left(u^{2}+i u\right)}
\end{equation}
\end{minipage}
\noindent\begin{minipage}{.3\linewidth}
\begin{equation}
g_{1}:=\frac{\xi+d}{\xi-d}
\end{equation}
\end{minipage}
\end{lem}

From this expression, one can compute the analytic gradients of the option price in terms of the derivatives of the characteristic function. Then they can be used to optimize an appropriate objective function using gradient-descent based methods (See chapter \ref{chap:optimization_problem}) but, as will be explained in the next section, the analytic derivatives have not been widely used traditionally to calibrate Heston models prior to Cui et al. due to the complexity of the expressions obtained by derivating the available characteristic function expressions as eq \ref{eq:char_heston}  \cite{cui17}.

\subsection{Calibration Difficulties}
As opposed to simpler 1-dimensional models, Heston model calibrations is a multidimensional optimization problem with 5 degrees of freedom. Furthermore, the structure of this optimization problem is not known.

According to \cite{cui17} no consensus exists among researcher regarding whether the objective function of this optimization problem is convex or not. Some results point to a non-convex function, as the calibration methods proposed in \cite{che07, mik03} (which yielded different results for different initial points) and one must use long or short term approximations and rules to provide a sufficient initial guess. Recent research claims to provide methods that reach a unique solution independently of the initial point \cite{ger12} which, according to that study this indicates some structure that, even if not necessarily convex, tends to lead an initial guess to a stationary result.
There's also no consensus on whether the problem always has a single  optimum. In particular, it is known that there exists dependencies between the parameters that yield to similar results. For example, $lim_{t \rightarrow \infty} Var(\nu_t) = \dfrac{\sigma^2 \overline{\nu}}{\kappa}$, so large values of $\kappa$ and $\sigma$ can provide a model that prices options similarly to one with proportionally smaller values of these two parameters. The study by Cui et al. \cite{cui17} defends that this yields to the the objective function of the optimization problem flat close to the optimum.

As said above, there's no guarantee that a gradient-based method converges to the global optimum of the model parameters, but even obtaining a local optimum has been traditionally difficult. A lot of literature uses numerical gradients\cite{ger12} for these methods when trying to solve the Heston calibration problem (which are less accurate and more computationally consuming), because no simple analytic gradients were available and the ones obtained with symbolic algebra packages from the expressions of the characteristic function were intractable.

Prior to the work of Cui et al. \cite{cui17}, the existing methods could be summarized as:

\begin{itemize}
\item{\textbf{Heuristic based models:}}
Using the relationships stated in \ref{hes_param_properties}, some studies reduce the dimension of the optimization problem by assuming some values or relationships between the parameters from the observation of a specific volatility surface. For example, Gatheral sets $\nu_0$ to the short-term ATM implied variance obtained by using a BS model \cite{gat06}, an heuristic further justified by Chen \cite{che07}, where the linearity between $\nu_0$ and the BS implied volatility was verified for short maturities (less than 2 months). Other heuristics used in the industry are $\kappa = \dfrac{2.75}{\tau}$ and setting $\overline{\nu}$ to the BS short term volatility \cite{cla11}.

These assumptions may restrict the optimization problem domain and exclude the optimum.
2.2.2.
\item{\textbf{Stochastic methods:}}
They are usually used in combination with deterministic search methods, as the Nelder and Mead simplex method \cite{lag98} and would avoid the pitfalls of the gradient-based methods if the optimization problem is not convex. Some examples are Wang-Landau \cite{che07}, differential evolution and particle swarm \cite{gil12_2}, and simulated annealing \cite{moo09}.
These methods are too computationally expensive for real-time use as of now: Fernandez et al. use GPU computations to calibrate options using a SV model called SABR, and it took 421.72 seconds to calibrate 12 instruments with tolerance of $10^{-2}$ using 2 NVIDIA Geforce GTX470 GPUs \cite{fer13}.

\item{\textbf{Deterministic methods:}}

\end{itemize}

\section{Calibration as an optimization problem} \label{chap:optimization_problem}
The goal of calibrating a model using market data, is to obtain model parameters that, when used for option valuation with an appropriate option valuation method, yield results similar to the real market option prices.

Completely matching the market for all strikes and maturities is an over-determined problem so, generally, the calibration consists on a least squares minimization problem, using a set of option market prices at different strikes and maturities.

In this section, the objective function of the optimization problem used by \cite{cui17} is presented, as it will be the one used in this study.

\subsection{Objective Function}
Let $C^*(K_i, \tau_i)$ be the market price of a European call option and
 $C(\boldsymbol{\theta}; K_i, \tau_i)$ be the price at the same strikes and maturities obtained by using the Heston analytic formula \ref{eq:heston_analytic}. Let's also assume that we use n different options to calibrate the model, so 
 $i \in [1,n] \subset \Z$
 . Then:
 \begin{definition} The calibration of the model is defined as the minimization problem $$min_{\boldsymbol{\theta} \in \R^5} (f(\boldsymbol{\theta}))$$ $$f(\boldsymbol{\theta}) := \dfrac{1}{2}||\boldsymbol{r}(\theta)||^2 = \dfrac{1}{2} \boldsymbol{r}^T(\boldsymbol{\theta})\boldsymbol{r}(\boldsymbol{\theta})$$
\end{definition}

, where $\boldsymbol{r}(\boldsymbol{\theta})$ is the n-dimensional vector of the residuals obtained when pricing the options considered for calibration using the model parameters. That is:

\begin{align}
\boldsymbol{r}(\boldsymbol{\theta}) := \left[r_1(\boldsymbol{\theta}), \ldots, r_n(\boldsymbol{\theta}) \right]^T, 
&& r_i(\boldsymbol{\theta}) :=  C(\boldsymbol{\theta}; K_i, \tau_i) - C^*(K_i, \tau_i), && i = 1, \ldots, n
\end{align}

It is worth noting that the choice of a specific objective function (and appropriate strikes and maturities to use as calibration points) will intimately characterize the model. This can be used when trading to fit to the specific needs of trader, ultimately leading to different objective functions, as relative price differences instead of absolute ones, or log-likelihood based methods \cite{jac00} for, for example, hedging and market making \cite{chr02}. 

One must be cautious, as this choice can lead to consistently giving more weight in the calibration to some set of options over the others. For example, according to \cite{cui17}, a commonly used minimization problem in industry for calibration consists on a least-squares problem similar to the one used in this study, using squared differences of the implied BS volatility of each calibration option, which can lead to giving more weight to OTM options in comparison with ITM ones.

The choice of calibration strikes and maturities can also have a similar impact in the pricing model. For example, as an option gets closer to the expiration date, the variance of the price change until expiration of the underlying asset gets smaller. So, if one were to always use the same strikes across different expiries, then for long-term maturities the model would end up with a lot of calibration points ATM but would lack control points in regions where the density function of the underlying asset is still relevant, which could lead to serious pricing errors for mildly OTM and ITM options. For the short-term maturities, the relevant regions of the density function would have small number of sparse strikes and there would be a lot of strikes in very OTM and ITM regions, where prices are mostly close to 0 and to the discounted payout respectively, so they are less sensitive to the change in the model parameters and do not help much in calibration (the price derivatives with respect to the model parameters will be almost flat at those expiry-maturity points, so they will not affect much the optimization problem), effectively wasting computational power.

As this study does not strive for any specific trading strategy, the choice of TODO: finish this and mention that the calibration objective function should be the same as the one used when valuating the model performance \cite{chr02}

%% Anava per aqui1

\section{Characteristic function and Gradient Calculation} \label{sec:gradient}

For long-term maturities (i.e. big values of $\tau$), Kahl and J\"{a}ckel show that the characteristic  \ref{eq:char_heston} has discontinuities as $u$ increases \cite{kah05}, which can lead to numerical problems for many option valuation methods based on the integral expression \ref{eq:integral_option_valuation}. They show that this discontinuities arise because of a term in equation \ref{eq:char_heston} of the form $G^\alpha(u) := e^{\alpha log G(u)}$ with $G(u) := \dfrac{1 - g_1 e^{d\tau}}{1 - g_1}$ and $\alpha := \dfrac{\kappa \overline{\vega}}{\sigma^2}$, for non-integer values of $\alpha$. This is due to the spiral shape of $G(u)$ which produces a phase jump on $log(G(u))$ each time $G(u)$ crosses the negative side of the real line. This only happens if $d$ is defined taking the principal value of the square root that appears in its expression (see equation \ref{eq:char_d}) and the discontinuity can be avoided if the second value is used \cite{alb07}.


%% Anava per aqui2

$\quad A:=\frac{A_{1}}{A_{2}}$
$\quad A_{1}:=\left(u^{2}+i u\right) \sinh \frac{d \tau}{2}$
$\quad A_{2}:=\frac{d}{v_{0}} \cosh \frac{d \tau}{2}+\frac{\xi}{v_{0}} \sinh \frac{d \tau}{2}$
\begin{equation}
B:=\dfrac{de^{\kappa \tau / 2}}{v_0A_2}
\end{equation}

\section{Multi Resolution Analysis and Shannon Wavelets}

\subsection{Multi Resolution Analysis} \label{def:mra}
Multi Resolution Analysis (MRA) is a method that ultimately allows to express any function in $L^2(\R)$ using a countable orthogonal family of wavelets. This family can then be truncated into a finite family and the original function can be orthogonally projected into the resulting subspace, obtaining an approximation with a certain level of resolution. Increasing the considered number of members of the wavelet family will increase the resolution of the approximation, converging to a perfect representation when all the wavelets are used \cite{tour}.

Given the space $L^2(\R) = \left\{f: \int_{-\infty}^{\infty}{|f(x)|^2 dx < \infty} \right\}$, a Multi Resolution Analysis is defined as 
a family of nested successive approximation closed spaces:
$$ \cdots \subset V_{-2} \subset V_{-1} \subset V_0 \subset V_1 \subset V_2 \subset \cdots $$

Where the subpsaces $V_i$ are complete (they are not redundant and cover $L^2(\R)$:
$$\overline{\bigcup_{i\in{\Z}}{V_i}} = L^2(\R) \text{, and } \bigcap_{m\in{\Z}} = {0}$$

, they have self-similarity in scale (all spaces are geometric scalings of $V_0$ by powers of 2):
$$ f(x) \in V_i \Leftrightarrow f(2x) \in V_{i + 1} $$

, they have self-similarity in time:
$$ f(x) \in V_0 \Rightarrow f(x - k) \in V_0, \forall k \in \Z $$

(Note that self-similarity in scale implies that the self-similarity in time translates to all spaces $V_i$ as $f(x) \in V_i \rightarrow f(x - 2^i k) \in V_i$.), and the integer shifts of a (or a finite group of) generator function $\phi$ form an orthogonal basis of $V_0$. 

In summary, we can define:

\begin{definition} \label{def:mra} (MRA): Consider $\phi \in L^2(\R)$ a wavelet that spans the family $\{\phi_{m,k}\}m,k\in\Z$ defined as the normalized scaled integer shifts of $\phi$. That is, $\phi_{m,k} = 2^{m/2}\phi(2^m x - k)$, and let $V_m := closure_{L^2(\R)}\left\langle\{\phi_{m,k}\}_{k \in \Z}\right\rangle$ 
\end{definition}

Then, if $\phi$ and $V_m$ fulfill the conditions above, we say that $\phi$ is the scaling function or father wavelet of the MRA $\{V_m\}$ (note that the previous definition properly defines $\{V_m\}$ as a sequence of nested subspaces and that $\phi$ provides an orthonormal basis for each of them).

One of the important implications of obtaining a father wavelet and its MRA is that another wavelet family can be obtained from it, which will be a basis of $L^2(\R)$. In order to do that, let's consider the set of subspaces $W_m$ such that $V_{m+1} = V_m \oplus W_m$. Then $L^2(\R) = \sum_m{\oplus W_m}$ and there exists a function $\psi \in W_0$ (called mother wavelet) that generates an orthonormal basis of $L^2(\R)$ \cite{dau92} by defining the wavelet functions:
$$ \psi_{m, k} = 2^{m/2}\psi(2^m x - k)$$
. Note that each $\{W_{m,k}\}_{k \in \Z}$ gives an orthonormal basis of $W_m$, and $\{W_{m_k}\}_{m\in [-\infty, m-1], k \in \Z}$ is an orthonormal basis of $V_m$. So for any $m \in \Z$ we can define a projection mapping $P_m$ from any function $f \in L^2(\R)$ into $V_m$:
$$ P_m f(x) = \sum_{j = -\infty}^{m-1} \sum_{k \in \Z} d_{j,k} \psi_{j, k}(x) = \sum_{k \in \Z} c_{m, k} \phi_{m, k}(x)$$
, where $d_{j, k} = \left\lbrace f,\psi_{j, k}\right\rbrace$, $c_{m, k} = \left\lbrace f,\phi_{m, k}\right\rbrace$, and $\left\lbrace f,g\right\rbrace = \int_\R f(x) \overline{g(x)} dx$. Further, this projection converges in the $L^2$ norm as m tends to infinity \cite{tour}.

\subsection{Shannon Wavelets}
Claude Shannon introduced the usage of the cardinal sine function for information modeling \cite{sha49}:
$$ sinc(x) := \left\{ \begin{array}{rcl} \dfrac{sin(\pi x)}{\pi x} & \mbox{for} & x \neq 0 \\ 1 & \mbox{for} & x = 0 \end{array}\right.$$

This function serves as the father wavelet from which we obtain the families $\phi_{m, k}$ and $\psi_{m_k}$:
$$\begin{array}{rcl}
\phi_{m,k}(x) = 2^{m/2} \dfrac{sin(\pi (2^m x - k))}{\pi (2^m x - k)}, & k \in \Z
\end{array}$$
$$\begin{array}{rcl}
\psi_{m,k}(x) = 2^{m/2} \dfrac{sin(\pi (2^m x - k - 1/2)) - sin(2 \pi (2^m x - k - 1/2))}{\pi (2^m x - k - 1/2)}, & k \in \Z
\end{array}$$

Shannon wavelets are interesting in part because while they have a slow decay in time domain, they have a simple expression and sharp compact support in the frequency domain.

$$\begin{array}{rcl}
\hat{\phi}_{m,k}(x) = \dfrac{e^{-i k/2^m w}}{2^{m/2}}rect \left(\dfrac{w}{2^{m+1}\pi}\right), & k \in \Z
\end{array}$$
$$\begin{array}{rcl}
\hat{\psi}_{m,k}(x) = -\dfrac{e^{-i \dfrac{k + 1/2}{2^m} w}}{2^{m/2}} \left(rect \left(\dfrac{w}{2^{m}\pi} - \dfrac{3}{2}\right) + rect \left(-\dfrac{w}{2^{m}\pi} - \dfrac{3}{2}\right) \right), & k \in \Z
\end{array}$$

When using Shannon Wavelets to approximate a function with a truncated wavelet expansion \cite{mar17} shows a bound for the projection error into $V_m$, by using concepts of band-limited functions.

\begin{definition} A function f is called band-limited if $\exists B \in \R^+$, with $B < \infty$ such that
$$
f(x) = \dfrac{1}{2\pi} \int_{-B \pi}^{B \pi} \hat{f}(\omega) e^{i\omega x}d\omega 
$$
, that is, the support of $\hat{f}$ is contained in the interval $[-B, B]$. The parameter B is referred to as the bandwidth of f.
\end{definition}

The nested subspaces of a Shannon MRA can be expressed in terms of band-limited functions because of the sinc Fourier transform rectangular shape, as stated in the following lemma from \cite{ste11}

\begin{lem} Consider an MRA generated from the Shannon scaling function as defined in \ref{def:mra}, then each subspace $V_m$ corresponds to the space of functions $f \in L^2(\R)$ with bandwidth $B \leq 2^m$
\end{lem}

Combining this lemma with the $L^2$ convergence of the projections $P_m$ of $f$ into $V_m$ yields the following corollary \cite{mar17}

\begin{cor} The orthogonal projection $P_m$ of a Shannon MRA is equivalent to:
$$
P_m f(x) = \dfrac{1}{2 \pi} \int_{-2^m \pi}^{2^m \pi} \hat{f}(\omega) e^{i \omega x} d \omega
$$
\end{cor}

Which, in turn, can be used to derive the following bound to the error of the orthogonal projection.

\begin{definition}Given $f \in L^2(\R)$, let $H(\xi)$ be:
$$
H(\xi) := \dfrac{1}{2 \pi} \int_{|\omega| > \xi}\left|\hat{f}(\omega)\right| d\omega
$$
, the normalized mass of the two-side tails of $\hat{f}$ defined by $\xi$.
\end{definition}

\begin{lem} Let $\epsilon_m(x) := f(x) - P_m f(x)$ (the pointwise approximation error due to the projection of f into $V_m$). Then $|\epsilon_m(x)| \leq H(2^m \pi)$ \cite{mar17}
\end{lem}

\section{SWIFT}

Given a certain stochastic model for the underlying log returns price evolution over time and its density function f (equation \ref{eq:integral_option_valuation}) chosen to valuate the options and its correspondent characteristic function $\hat{f}(w) = \int_{\R} e^{-iwx} f(x) dx$, Shannon Wavelets Inverse Fourier Technique combines Shannon Wavelets and $\hat{f}$ to truncate f at a fixed approximation level m and provide a fast way to obtain the Fourier coefficients and recover the time domain ones through fast Fourier transform. (TODO: Cite)

\subsection{Coefficients via Vieta's formula}
$$
c_{m,k} \approx c^*_{m,k} = \dfrac{2^{m/2}}{2^{J-1} \sum_{j=1}^{2^{J-1}}} \R \left[\hat{f}\left(\dfrac{(2j - 1) \pi 2^m}{2^J}\right) e^{\dfrac{ik\pi(2j-1)}{2^J}}\right] 
$$

\subsection{Sinc Integral}
SWIFT works with the cardinal sinc function and it needs to evaluate it's integral $Si(t) = \int_0^t{sinc(x) dx}$ for which there's no closed form. This problem was addressed in \cite{abr15_1, abr15_2} by using a combination of Vieta's formula and a cosine product-to-sum identity, resulting in an approximation by an incomplete cosine expansion


\section{Formulation of the study}

In this study a Heston model will be calibrated with a a Levenberg–Marquardt method with the objective function defined in \ref{eq:objective_function}. The calibration points will be obtained from real marked data and will be taken at expiries (TODO: add expiries when they are decided) to take into account both short-term and long-term maturities. The strikes will be taken at 0\%, $\pm 25 \% $, and $\pm 50 \%$ BS delta, as recommended in section \ref{sec:optimization_problem}.

The option valuation will be done with a SWIFT method, using the expression for the characteristic function and it's derivatives provided in \cite{cui17}. These derivatives are taken from an exact expression of the option price. In order for the difference between the real derivatives and the derivatives of the Shannon Wavelet approximation to be small, the approximation error will be restricted accordingly.

\subsection{Robustness tests}
There is no guarantee the objective function of the calibration problem is convex, so local minima problems can arise so, in order to both validate the robustness of the method proposed and assert its region of attraction, several convergence tests will be run.

Consider a set of initial parameter configurations $\{\boldsymbol{\theta}^*\}$. Given an initial parameter vector $\boldsymbol{\theta}_{i}^*$ its correspondent call option prices will be computed at the control points $C(K_j, \tau_j)$. Then, random perturbations will be applied to each initial parameters vector to obtain a set of transformed parameters
$\{\boldsymbol{\theta}_i\}$. The proposed calibration methodology will then be applied to these parameters and the convergence region will be studied.

\subsection{Speed tests}
The SWIFT approach should drastically speed up the computation time with respect to the original option valuation method used in the study of Cui et al. \cite{cui17}. In order to quantify this speed improvement several sets of Heston model parameters and market parameters (at both long and short term expiries and OTM and ITM options, to provide an heterogeneous sample of contracts) will be generated. For these inputs, the price of their correspondent option price will be computed using both the swift method and a simple quadrature numerical integration as in \cite{cui17}.

\subsection{Fit tests}
The Heston model should capture the market dynamics like skewness and high-kurtosis better than other simpler models like Black and Scholes. Comparing directly the goodness-of-fit of a Heston model and a BS one should show the clear dominance of the first one, but this could be dominated by the fact that BS has only 1 model parameter while Heston has 5.

This simple test above will be run, but it is of more interest to also compare the performance of a hibrid BS model, where the BS volatilities are computed at specific control points and the whole theoretic volatility smile is computed from them. To render both BS and Heston comparable, 5 BS volatility smile cutoff points will be chosen. These will be chosen for the same expiry (in hopes that the BS cutoff points can cover enough of the volatility smile). For this scenario, several Heston calibration points will be chosen and both models will be calibrated using the same minimum squares optimization problem and Levenberg–Marquardt method.

Christoffersen and Jacobs \cite{chr02} show the importance of using the same objective function for both model calibration and goodness-of-fit tests, and to use the same one across the different models we want to compare. If, instead, the vanilla BS model was calibrated by using a volatility based method, one could reduce the fitness over the evaluation objective function.

\subsection{General minimum}
Finally we will compare the results with the results obtained from a derivative-free optimization scheme that, even if slower, will avoid local minimums if repeated enough times.

\section{Numerical Results}

\section{Conclusions}

\section{Bibliography}

%\newpage

\bibliography{biblio}{}
\bibliographystyle{plain}

%______________________________________________________________
\appendix
\vfill\newpage \section{Title of the appendix}
You can include here an appendix with details that can not be included in the core of the document. You should reference the sections in this appendix in the core document.

\end{document}